{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fd6105fd",
   "metadata": {},
   "source": [
    "# Code to Produce an Output from the Models\n",
    "Presumably this will need to be transferred to a script-based format but this is the code to turn an input image into a controller action"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "040f6a89",
   "metadata": {},
   "source": [
    "## Loading Things in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "caa3843c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Conv2D, Conv2DTranspose, Flatten, Input, Dense, Dropout, Lambda, Reshape, MaxPooling2D, LSTM, Reshape\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a4f7dda8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# variables for potential modification\n",
    "img_height = 128 # this and all below variables should be the same for the trained images and input images\n",
    "img_width = 128\n",
    "num_channels = 1\n",
    "input_shape = (img_height, img_width, num_channels)\n",
    "z_len = 2048 # the length of the image compression made by the encoder\n",
    "a_len = 1 # the length of the action vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "be1ead55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model architectures\n",
    "\n",
    "# ==========VAE========================\n",
    "\n",
    "# load the vae (have to make the architecture again, make sure the code below\n",
    "#   matches the code in the Data Prepper/VAE Trainer)\n",
    "\n",
    "\n",
    "# ====== Encoder ======\n",
    "# changing this will make the model exponentially larger or smaller\n",
    "latent_dim = 2048\n",
    "\n",
    "# the model (saved in x)\n",
    "input_img = Input(shape=input_shape, name='encoder_input')\n",
    "x = Conv2D(64, 3, padding='same', activation='relu')(input_img)\n",
    "x = MaxPooling2D((2,2), padding = 'same')(x)\n",
    "x = Dropout(0.2)(x)\n",
    "x = Conv2D(128, 3, padding='same', activation='relu')(x)\n",
    "x = MaxPooling2D((2,2), padding = 'same')(x)\n",
    "x = Dropout(0.2)(x)\n",
    "x = Conv2D(64, 3, padding='same', activation='relu')(x)\n",
    "x = MaxPooling2D((2,2), padding = 'same')(x)\n",
    "x = Dropout(0.2)(x)\n",
    "x = Conv2D(32, 3, padding='same', activation='relu')(x)\n",
    "\n",
    "conv_shape = K.int_shape(x) #Shape of conv to be provided to decoder\n",
    "#Flatten\n",
    "x = Flatten()(x)\n",
    "x = Dense(latent_dim*2, activation='relu')(x)\n",
    "\n",
    "# Two outputs, for latent mean and log variance (std. dev.)\n",
    "#  Use these to sample random variables in latent space to which inputs are mapped. \n",
    "z_mu = Dense(latent_dim, name='latent_mu')(x)   #Mean values of encoded input\n",
    "z_sigma = Dense(latent_dim, name='latent_sigma')(x)  #Std dev. (variance) of encoded input\n",
    "\n",
    "#REPARAMETERIZATION TRICK\n",
    "# Define sampling function to sample from the distribution\n",
    "# Reparameterize sample based on the process defined by Gunderson and Huang\n",
    "# into the shape of: mu + sigma squared x eps\n",
    "#This is to allow gradient descent to allow for gradient estimation accurately. \n",
    "def sample_z(args):\n",
    "    z_mu, z_sigma = args\n",
    "    eps = K.random_normal(shape=(K.shape(z_mu)[0], K.int_shape(z_mu)[1]))\n",
    "    return z_mu + K.exp(z_sigma / 2) * eps\n",
    "\n",
    "# sample vector from the latent distribution\n",
    "# z is the labda custom layer we are adding for gradient descent calculations\n",
    "  # using mu and variance (sigma)\n",
    "z = Lambda(sample_z, output_shape=(latent_dim, ), name='z')([z_mu, z_sigma])\n",
    "\n",
    "#Z (lambda layer) will be the last layer in the encoder.\n",
    "# Define and summarize encoder model.\n",
    "encoder = Model(input_img, [z_mu, z_sigma, z], name='encoder')\n",
    "\n",
    "# ==== Decoder ====\n",
    "\n",
    "# decoder takes the latent vector as input\n",
    "decoder_input = Input(shape=(latent_dim, ), name='decoder_input')\n",
    "\n",
    "# Need to start with a shape that can be remapped to original image shape as\n",
    "#we want our final utput to be same shape original input.\n",
    "#So, add dense layer with dimensions that can be reshaped to desired output shape\n",
    "x = Dense(conv_shape[1]*conv_shape[2]*conv_shape[3], activation='relu')(decoder_input)\n",
    "# reshape to the shape of last conv. layer in the encoder, so we can \n",
    "x = Reshape((conv_shape[1], conv_shape[2], conv_shape[3]))(x)\n",
    "# upscale (conv2D transpose) back to original shape\n",
    "# use Conv2DTranspose to reverse the conv layers defined in the encoder\n",
    "x = Conv2DTranspose(32, 3, padding='same', activation='relu',strides=(2, 2))(x)\n",
    "x = Conv2DTranspose(32, 3, padding='same', activation='relu')(x)\n",
    "x = Conv2DTranspose(64, 3, padding='same', activation='relu',strides=(2, 2))(x)\n",
    "x = Conv2DTranspose(64, 3, padding='same', activation='relu')(x)\n",
    "x = Conv2DTranspose(64, 3, padding='same', activation='relu',strides=(2, 2))(x)\n",
    "#Can add more conv2DTranspose layers, if desired. \n",
    "#Using sigmoid activation\n",
    "x = Conv2DTranspose(num_channels, 3, padding='same', activation='sigmoid', name='decoder_output')(x)\n",
    "\n",
    "# Define and summarize decoder model\n",
    "decoder = Model(decoder_input, x, name='decoder')\n",
    "\n",
    "# apply the decoder to the latent sample \n",
    "z_decoded = decoder(z)\n",
    "\n",
    "# ===== Loss Function =====\n",
    "\n",
    "class CustomLayer(keras.layers.Layer):\n",
    "    def vae_loss(self, x, z_decoded):\n",
    "        x = K.flatten(x)\n",
    "        z_decoded = K.flatten(z_decoded)\n",
    "        \n",
    "        # Reconstruction loss (as we used sigmoid activation we can use binarycrossentropy)\n",
    "        recon_loss = keras.metrics.binary_crossentropy(x, z_decoded)\n",
    "        \n",
    "        # KL divergence\n",
    "        kl_loss = -5e-4 * K.mean(1 + z_sigma - K.square(z_mu) - K.exp(z_sigma), axis=-1)\n",
    "        return K.mean(recon_loss + kl_loss)\n",
    "\n",
    "    # add custom loss to the class\n",
    "    def call(self, inputs):\n",
    "        x = inputs[0]\n",
    "        z_decoded = inputs[1]\n",
    "        loss = self.vae_loss(x, z_decoded)\n",
    "        self.add_loss(loss, inputs=inputs)\n",
    "        return x\n",
    "\n",
    "# apply the custom loss to the input images and the decoded latent distribution sample\n",
    "y = CustomLayer()([input_img, z_decoded])\n",
    "# y is basically the original image after encoding input img to mu, sigma, z\n",
    "# and decoding sampled z values.\n",
    "#This will be used as output for vae\n",
    "\n",
    "# ===========RNN===========================\n",
    "\n",
    "# Layers\n",
    "input_to_rnn = Input(shape=(1,z_len))\n",
    "\n",
    "x = LSTM(z_len, return_sequences=True)(input_to_rnn)\n",
    "x = Dropout(0.2)(x)\n",
    "x = Dense(z_len)(x)\n",
    "x = Dropout(0.2)(x)\n",
    "\n",
    "rnn_output = Dense(2048, activation='sigmoid')(x)\n",
    "\n",
    "# ============Controller=========================\n",
    "\n",
    "# Layers\n",
    "\n",
    "input_to_controller = Input(shape=(1, z_len*2))\n",
    "\n",
    "x = Dense(z_len)(input_to_controller)\n",
    "x = Dropout(0.2)(x)\n",
    "x = Dense(z_len/2)(x)\n",
    "x = Dropout(0.2)(x)\n",
    "x = Dense(z_len/4)(x)\n",
    "x = Dropout(0.2)(x)\n",
    "x = Dense(z_len/16)(x)\n",
    "x = Dropout(0.2)(x)\n",
    "\n",
    "ctrl_output = Dense(a_len, activation='sigmoid')(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6e09cc29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model loading\n",
    "# load encoder\n",
    "vae = Model(input_img, y, name = 'vae')\n",
    "vae.load_weights(os.getcwd() + \"\\\\models\\\\vae.h5\")\n",
    "encoder = Model(vae.input, vae.layers[15].output)\n",
    "# load rnn and controller\n",
    "rnn = Model(input_to_rnn, rnn_output, name = 'rnn')\n",
    "rnn.load_weights(os.getcwd() + \"\\\\models\\\\rnn.h5\")\n",
    "ctrl = Model(input_to_controller, ctrl_output, name = 'controller')\n",
    "ctrl.load_weights(os.getcwd() + \"\\\\models\\\\cntrl.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4daf7d04",
   "metadata": {},
   "source": [
    "## Input/Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "72962ded",
   "metadata": {},
   "outputs": [],
   "source": [
    "# right now, will need to have saved an image to run it through the models\n",
    "#     edit img_path for desired image to process\n",
    "img_path = os.getcwd() + \"\\\\images\\\\2021-02-27\\\\2021-02-27-1350-01-24-22.NEF\"\n",
    "# loading in image and reshaping\n",
    "img_array = cv2.imread(img_path)\n",
    "img_array = cv2.cvtColor(img_array, cv2.COLOR_BGR2GRAY)\n",
    "img_array = cv2.resize(img_array, (img_height, img_width))\n",
    "img_array = img_array.reshape(-1, img_height, img_width, 1) # this 1 might need to be num_channels, but I'm not sure. If num_channels is ever not set to 1, keep an eye on this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "442d7c6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 1, 4096)\n"
     ]
    }
   ],
   "source": [
    "# predictions by each piece of the model\n",
    "z = encoder.predict(img_array) # encode image\n",
    "z = z.reshape(-1, 1, 2048) # reshape for rnn\n",
    "zprime = rnn.predict(z) # make prediction\n",
    "z_and_zprime = np.reshape(np.concatenate((z[0][0], zprime[0][0])), (1, z_len*2))[None,:,:] # concat for controller\n",
    "action = ctrl.predict(z_and_zprime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6bdb6f18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "z: [[[ 23.53091  114.87907  102.02956  ... -94.815125 -53.232956 108.08724 ]]]\n",
      "z': [[[0.16432846 0.29797998 0.32640016 ... 0.4681464  0.48924953 0.26297385]]]\n",
      "action: [[[1.]]]\n"
     ]
    }
   ],
   "source": [
    "# results\n",
    "print(\"z:\", z)\n",
    "print(\"z':\", zprime)\n",
    "print(\"action:\", action)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
