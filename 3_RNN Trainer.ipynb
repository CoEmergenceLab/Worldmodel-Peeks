{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Trainer\n",
    "(As opposed to the controller part of the model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading stuff into the kernel\n",
    "Imports and training data (don't need the encoder, train data is already converted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import tensorflow.compat.v1.keras.backend as K\n",
    "import tensorflow as tf\n",
    "tf.compat.v1.disable_eager_execution()\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import Conv2D, Conv2DTranspose, Input, Flatten, Dense, Lambda, Reshape, MaxPooling2D,LSTM, Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.models import save_model\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import joblib\n",
    "import cv2\n",
    "import random\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = joblib.load(\"images/train_data_rnn.z\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.0104093  -0.7574463   0.45002717 ... -1.2421685  -0.7610112\n",
      "  -1.6004298 ]]\n",
      "[[-0.87825227 -0.3997757  -1.2685652  ... -0.304825    0.14091796\n",
      "  -0.38791984]]\n",
      "(1, 2048)\n",
      "2048\n"
     ]
    }
   ],
   "source": [
    "train_data = np.array([np.array(p[0])for p in data])\n",
    "answers = np.array([np.array(p[1]) for p in data])\n",
    "print(train_data[0])\n",
    "print(answers[0])\n",
    "print(np.shape(train_data[0]))\n",
    "z_len = np.shape(train_data[0])[-1]\n",
    "print(z_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Layers\n",
    "input_to_rnn = Input(shape=(1,z_len))\n",
    "\n",
    "x = LSTM(z_len, return_sequences=True)(input_to_rnn)\n",
    "x = Dropout(0.2)(x)\n",
    "x = Dense(z_len)(x)\n",
    "x = Dropout(0.2)(x)\n",
    "\n",
    "output = Dense(2048, activation='sigmoid')(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn = Model(input_to_rnn, output, name='rnn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"rnn\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_2 (InputLayer)        [(None, 1, 2048)]         0         \n",
      "                                                                 \n",
      " lstm_1 (LSTM)               (None, 1, 2048)           33562624  \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 1, 2048)           0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 1, 2048)           4196352   \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 1, 2048)           0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 1, 2048)           4196352   \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 41,955,328\n",
      "Trainable params: 41,955,328\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "rnn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn.compile(optimizer='adam', loss='mean_squared_error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 2048)\n"
     ]
    }
   ],
   "source": [
    "print(np.shape(answers[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 602 samples, validate on 151 samples\n",
      "Epoch 1/10\n",
      "602/602 [==============================] - 11s 18ms/sample - loss: 0.9766 - val_loss: 0.8943\n",
      "Epoch 2/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mattp\\anaconda3\\lib\\site-packages\\keras\\engine\\training_v1.py:2057: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates = self.state_updates\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "602/602 [==============================] - 4s 7ms/sample - loss: 0.9074 - val_loss: 0.8582\n",
      "Epoch 3/10\n",
      "602/602 [==============================] - 4s 7ms/sample - loss: 0.8828 - val_loss: 0.8284\n",
      "Epoch 4/10\n",
      "602/602 [==============================] - 4s 7ms/sample - loss: 0.8661 - val_loss: 0.8260\n",
      "Epoch 5/10\n",
      "602/602 [==============================] - 4s 7ms/sample - loss: 0.8558 - val_loss: 0.8212\n",
      "Epoch 6/10\n",
      "602/602 [==============================] - 4s 7ms/sample - loss: 0.8484 - val_loss: 0.8195\n",
      "Epoch 7/10\n",
      "602/602 [==============================] - 4s 7ms/sample - loss: 0.8433 - val_loss: 0.8195\n",
      "Epoch 8/10\n",
      "602/602 [==============================] - 4s 7ms/sample - loss: 0.8384 - val_loss: 0.8201\n",
      "Epoch 9/10\n",
      "602/602 [==============================] - 4s 7ms/sample - loss: 0.8332 - val_loss: 0.8174\n",
      "Epoch 10/10\n",
      "602/602 [==============================] - 4s 7ms/sample - loss: 0.8267 - val_loss: 0.8218\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1ddadc5cee0>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# after testing with z length = 2048, 10 epochs seems like the best. val_loss goes from 0.90 to 0.82, but goes back to\n",
    "#   back to 0.83 for overfitting. If more data added, could potentially do more epochs\n",
    "#   also, doesn't take that long to run due to small dimensionality of inputs and outputs\n",
    "rnn.fit(train_data, answers, epochs=10, verbose = 1, batch_size = 32, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn.save_weights('models/rnn.h5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
